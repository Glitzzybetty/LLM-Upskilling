{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "490d8b19-e433-4adc-9f8b-d955b0ee492a",
   "metadata": {},
   "source": [
    "## PROJECT: Question-Answering on Private Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61647b8d-ab88-44de-994a-8d9389cedcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a34251c7-0ca4-4f3a-8ba6-fa4420927f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e90584e-b3cc-4fc3-9c1c-5bdda01a2601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install Docx2txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b6fdd48-c3d0-4c9c-a282-2a0ba070dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wikipedia -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f36bdf-6436-4f79-8ae0-a91cf142145c",
   "metadata": {},
   "source": [
    "## PDF Loader only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "842c08d7-6ede-4b09-8557-554ebb4916b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ensuring code is modular, start by defining a function\n",
    "## defining a funtion that only loads pdfs\n",
    "def load_document_pdf(file):\n",
    "    #takes pdf file and return it text\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    print(f'Loading {file}') ## leaving a message for the user\n",
    "    loader = PyPDFLoader(file)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac8eca5-a4d0-43a6-91c2-5e10ba3e20e8",
   "metadata": {},
   "source": [
    "## Other Transform loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "725f9d8c-1fe2-4134-8c49-ff1428132ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining a function that loads both pdfs and document, more ext can be added, e.g. csv, ht\n",
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}') ## leaving a message for the user\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}') ## leaving a message for the user\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    "        \n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3103b43-e3e4-483c-b8ac-890a7f75a909",
   "metadata": {},
   "source": [
    "## Public Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ff44cf-b13f-4d95-bb61-8c069e8b2e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a function that loads wikipedia article, \n",
    "# wikipedia, limit number of document to load with max docs\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query = query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "512b57ec-10d1-46c2-acfa-7052eb85ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#materials\n",
    "#https://python.langchain.com/docs/modules/data_connection/document_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef461e3-1a55-4f84-ad10-eadf5a162cd8",
   "metadata": {},
   "source": [
    "## Chunking or Splitting Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3537943d-e337-411c-bcda-560b1a425c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A function that rearrange document into chunks/ splits to regularise return texts by our AI model\n",
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks =  text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c393eeaa-2b80-464f-a7ac-f62f38921ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that calculate embedding cost beforehand\n",
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc= tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens/1000*0.0004:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490d656b-f294-40c3-b947-c0992382c902",
   "metadata": {},
   "source": [
    "### Embedding and Uploading to a Vector Databases (Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86189df7-16da-445c-b50e-ced4aa14b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function that creates embedding and uploads vectors into pinecone db\n",
    "def insert_or_fetch_embeddings(index_name):\n",
    "    import pinecone #pincone library\n",
    "    from langchain.vectorstores import Pinecone #Pincone class in langchain\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings #EMBEDs text to vectors\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "\n",
    "    if index_name in pinecone.list_indexes():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ...', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "        pinecone.create_index(index_name, dimension=1536, metric='cosine')\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('Ok')\n",
    "\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd5f42aa-0c33-4cb5-bef7-871fa24c3122",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function that delete indexes, as a single account user, I can use more than 1 index at once\n",
    "\n",
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "\n",
    "    if index_name == 'all':\n",
    "        indexes = pinecone.list_indexes()\n",
    "        print('Deleting all indexes...')\n",
    "        for index in indexes:\n",
    "            pinecone.delete_index(index)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ...', end=' ')\n",
    "        pinecone.delete_index(index_name)\n",
    "        print('Ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7ad14-9d58-45ff-b5c2-9998f8d7798c",
   "metadata": {},
   "source": [
    "### Asking and getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36c979ca-4339-4cd7-9d52-9d7851d302da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function that queries the vectors to get answers from loaded document\n",
    "def ask_and_get_answers(vector_store, q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':3})\n",
    "    \n",
    "    chains = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    \n",
    "    answer = chains.run(q)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e131c-3fd4-4dd2-8218-a2969f4a4e36",
   "metadata": {},
   "source": [
    "## Add Memory to App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db862e67-5786-499c-9a17-827d10d863c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_memory(vector_store, q, chat_history=[]):\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':3})\n",
    "\n",
    "    crc = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "    result = crc({'question':question, 'chat_history': chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "\n",
    "    return result, chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa6abd-31b3-4b01-9cc3-00e7c0b79cef",
   "metadata": {},
   "source": [
    "## Running Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7b297c1-0d68-45de-a417-a41f2d2a3815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/us_constitution.pdf\n"
     ]
    }
   ],
   "source": [
    "## testing pdf loader function\n",
    "data_pdf = load_document_pdf('files/us_constitution.pdf')\n",
    "# using indexes to print pages e.g.\n",
    "#print(data[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8464725e-22da-4e6c-bcdb-f1ea15a214f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'files/us_constitution.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "## to see metadata of pages e.g pg 1\n",
    "print(data_pdf[1].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cd3375c-69f5-4a22-8746-d248f1f32e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 19 pages in the data\n",
      "There are 3509 character in pg 13\n"
     ]
    }
   ],
   "source": [
    "#hOW MANY PAGES And characters in pg 13\n",
    "print(f'You have {len(data_pdf)} pages in the data')\n",
    "print(f'There are {len(data_pdf[13].page_content)} character in pg 13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e90fbdd-075d-43f9-b013-4f84fdd2b5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading https://www.uscis.gov/sites/default/files/document/guides/M-654.pdf\n"
     ]
    }
   ],
   "source": [
    "# to load remote pdf, add the url to the function\n",
    "remotdata = load_document('https://www.uscis.gov/sites/default/files/document/guides/M-654.pdf')\n",
    "#print(remotdata[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aea36748-94bd-422b-afad-70dc634d9732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its series of GPT foundation models. It was initially released on March 14, 2023, and has been made publicly available via the paid chatbot product ChatGPT Plus, and via OpenAI's API.  As a transformer-based model, GPT-4 uses a paradigm where pre-training using both public data and \"data licensed from third-party providers\" is used to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.: 2 Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions. GPT-4 is also capable of taking images as input on ChatGPT. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.\n",
      "\n",
      "\n",
      "== Background ==\n",
      " \n",
      "OpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\" It was based on the transformer architecture and trained on a large corpus of books. The next year, they introduced GPT-2, a larger model that could generate coherent text. In 2020, they introduced GPT-3, a model with 100 times as many parameters as GPT-2, that could perform various tasks with few examples. GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.\n",
      "Rumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.\n",
      "\n",
      "\n",
      "== Capabilities ==\n",
      "OpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\" They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,049 tokens respectively. Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to breaks in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input; this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams.To gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in natural language given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in JSON\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.When instructed to do so, GPT-4 can interact with external interfaces. For example, the model could be instructed to enclose a query within <search></search> tags to perform a web search, the result of which would be inserted into the model's prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using APIs, generating images, and accessing and summarizing webpages.A 2023 article in Nature stated programmers have found GPT-4 useful for assisting in coding tasks (despite its propensity for error), such as finding errors in existing code and suggesting optimizations to improve performance. The article quoted a biophysicist who found that the time he required to port one of his programs from MATLAB to Python went down from days to \"an hour or so\". On a test of 89 security scenarios, GPT-4\n"
     ]
    }
   ],
   "source": [
    "## look into wikipedia loader\n",
    "data = load_from_wikipedia('GPT-4')\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b19ecf37-cb09-4584-84e0-3e1165cffb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load_from_wikipedia('GPT-4', 'de') # german language code as second argument\n",
    "#print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2911c0e0-6db4-4922-8539-53250ca5e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/us_constitution.pdf\n",
      "You have 19 pages in the data\n",
      "There are 3509 character in pg 13\n"
     ]
    }
   ],
   "source": [
    "# calling the most recent load document function to test the chunk data function\n",
    "data = load_document('files/us_constitution.pdf')\n",
    "print(f'You have {len(data)} pages in the data')\n",
    "print(f'There are {len(data[13].page_content)} character in pg 13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84843e4e-c906-4d78-a244-2dec217d8a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 247 chunks\n",
      "Jersey four, Pennsylvania eight, Delaw are one, Maryland  \n",
      "six, Virginia ten, North Carolina five, South Carolina five, \n",
      "and Georgia three.  \n",
      "When vacancies happen in the Representation from any\n"
     ]
    }
   ],
   "source": [
    "#Calling the chunk data function on our loaded document\n",
    "chunks = chunk_data(data)\n",
    "print(f'You have {len(chunks)} chunks')\n",
    "print(chunks[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cab83ebd-e473-4b9b-9636-78892ad59c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 12912\n",
      "Embedding Cost in USD: 0.005165\n"
     ]
    }
   ],
   "source": [
    "# Know embedding cost before proceeding\n",
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41e6973f-5c61-4961-81b2-6a9db1008862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Desktop\\jupyter_ai\\venv\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes...\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "## using the delete function to delete any existing index\n",
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b171d39-4dc4-4ab5-969b-3ac63377e7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index askadocument and embeddings ...Ok\n"
     ]
    }
   ],
   "source": [
    "#Embedd the document and upload to a database like pinecone to store the vectors\n",
    "index_name = 'askadocument'\n",
    "vector_store = insert_or_fetch_embeddings(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "786165a8-7746-4862-803a-932860738a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document being referred to is the United States Constitution. It establishes the framework and principles of the government of the United States. It outlines the powers and limitations of the three branches of government (legislative, executive, and judicial), the rights and responsibilities of citizens, and the process for amending the Constitution.\n"
     ]
    }
   ],
   "source": [
    "### testing the function directly\n",
    "q = 'What is the whole document about?'\n",
    "answer = ask_and_get_answers(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad675154-8812-409c-a21e-dfa4dd5a5a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Quit or Exit to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #1:  What is the first Amendment of the US Constitution?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The First Amendment of the US Constitution guarantees the freedom of speech, religion, press, assembly, and the right to petition the government.\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #2:  Explain the concept of 'Federalism' as it is presented in the US Constitution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Federalism is a concept present in the US Constitution that establishes the division of powers between the national government and the state governments. It is the principle that powers and authority are shared between the two levels of government. Under federalism, certain powers are specifically delegated to the federal government, while other powers are reserved for the states or the people. This system allows for a balance of power and ensures that neither level of government becomes too powerful. It also allows for a degree of autonomy and decision-making at the state level while maintaining a unified country.\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #3:  Describe the Bill of Rights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The Bill of Rights refers to the first ten amendments to the United States Constitution. These amendments were ratified by the states on December 15, 1791. The Bill of Rights guarantees certain fundamental rights and liberties to all citizens. These rights include freedom of speech, religion, and the press; the right to bear arms; protection against unreasonable searches and seizures; the right to a fair trial; and protection against cruel and unusual punishment, among others. The Bill of Rights serves as a crucial safeguard of individual freedoms and limits the power of the government.\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #4:  Q1: how does the constitution address the issue of Presidential succession? Q2: Describe the bill of rights. Answer both questions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: A1: The Constitution addresses the issue of Presidential succession in Article II, Section 1, Clause 6, which states that in the event of the death, resignation, or removal of the President, the Vice President shall assume the office of the President.\n",
      "\n",
      "A2: The Bill of Rights refers to the first ten amendments of the United States Constitution. It guarantees various fundamental rights and liberties to the American people. These rights include freedom of speech, religion, and the press; the right to bear arms; protection against unreasonable searches and seizures; the right to a fair trial; and protection against cruel and unusual punishment, among others. The Bill of Rights serves to protect individual freedoms and limit the power of the government.\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #5:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quitting ... bye bye!\n"
     ]
    }
   ],
   "source": [
    "#creating a loop to allow user ask continuous question from the document\n",
    "import time\n",
    "i = 1\n",
    "print('Write Quit or Exit to quit')\n",
    "while True:\n",
    "    q = input(f'Question #{i}: ')\n",
    "    i = i+1 \n",
    "    if q.lower() in ['quit', 'exit']:\n",
    "        print('Quitting ... bye bye!')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    answer = ask_and_get_answers(vector_store, q)\n",
    "    print(f'\\nAnswer: {answer}') ##customising for readability\n",
    "    print(f'\\n{\"-\" * 50} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b99e8618-6dbf-4caf-8313-eb14997310b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes...\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "#delete_pinecone_index() ## doing this so i can create anothe index to pick from another document source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b45e39e-1459-4d00-a769-bd09984726b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index chatgpt and embeddings ...Ok\n"
     ]
    }
   ],
   "source": [
    "## testing other document source like wikipedia\n",
    "#data = load_from_wikipedia('ChatGPT', 'ro') ## in romania language, my native short code doesnt exist yet\n",
    "#chunks = chunk_data(data)\n",
    "#index_name = 'chatgpt'\n",
    "#vector_store = insert_or_fetch_embeddings(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3ae9eb4-7b29-42d0-a471-c98cd2de4d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT este un membru al familiei de modele de limbaj generative pre-antrenate (GPT). A fost reglat fin (o abordare a transferului de învățare) peste o versiune îmbunătățită a GPT-3 a OpenAI, cunoscută sub numele de „GPT-3.5”. Este un sistem care poate genera texte și poate fi folosit pentru a desfășura conversații cu utilizatori într-o varietate de domenii. Este utilizat în diverse scopuri, inclusiv pentru asistență și educație.\n"
     ]
    }
   ],
   "source": [
    "#q = 'Ce este Chatgpt?'\n",
    "#answer = ask_and_get_answers(vector_store, q)\n",
    "#print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ceb48a4-969e-4504-b717-112f45a7bf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are currently 27 amendments in the U.S. Constitution.\n",
      "[('How many amendments are in U.S. Constitution?', 'There are currently 27 amendments in the U.S. Constitution.')]\n"
     ]
    }
   ],
   "source": [
    "## asking with memory\n",
    "chat_history = []\n",
    "question = 'How many amendments are in U.S. Constitution?'\n",
    "result, chat_history = ask_with_memory(vector_store, q, chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "caf82ed7-8582-4f39-96ab-731eebc08650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of multiplying the number of amendments by 2 is not provided in the given context.\n",
      "[('How many amendments are in U.S. Constitution?', 'There are currently 27 amendments in the U.S. Constitution.'), ('Multiply the number of amendments by 2?', 'The result of multiplying the number of amendments by 2 is not provided in the given context.')]\n"
     ]
    }
   ],
   "source": [
    "question = 'Multiply the number of amendments by 2?'\n",
    "result, chat_history = ask_with_memory(vector_store, q, chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32235d19-2135-4bb0-9ffd-cdb8573761d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965cf88-330f-4da6-83a0-ed3ee2de8878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
